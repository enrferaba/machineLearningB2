{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# B2 Python practice \u2013 novice walk-through\n",
        "\n",
        "I am following the bulletin that sits next to the `B2_python.pdf` file. The idea is to keep every step tiny and clear so even someone that is opening Jupyter Notebook for the first time can copy it.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Titanic `train.csv`\n",
        "\n",
        "The teacher asked us to work with the `train.csv` sheet. I put it inside the **Machine Learning 1** folder so the notebook can open it without extra paths.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "import statistics\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "random.seed(90)  # the class rule\n",
        "\n",
        "csv_path = 'Machine Learning 1/train.csv'\n",
        "rows = []\n",
        "with open(csv_path, newline='', encoding='utf-8') as handle:\n",
        "    reader = csv.DictReader(handle)\n",
        "    for row in reader:\n",
        "        rows.append(row)\n",
        "\n",
        "len(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I like to peek at the first rows to make sure the file is correct and the accents look fine.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "for sample in rows[:5]:\n",
        "    print(sample['PassengerId'], sample['Name'], sample['Sex'], sample['Survived'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The target column we need to learn is **Survived**. Zero means the passenger did not make it. One means the passenger survived.\n",
        "\n",
        "I also counted the labels to see the class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "label_counts = Counter(int(r['Survived']) for r in rows)\n",
        "label_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare very small helper functions\n",
        "\n",
        "The bulletin asks for simple baselines (ZeroR, OneR) and a little k-NN. I am keeping all helpers inside the notebook so I can read them line by line.\n",
        "\n",
        "### 2.1. Feature preparation\n",
        "\n",
        "I am only going to use the most basic numeric columns: `Age`, `SibSp`, `Parch`, and `Fare`. Sex is important, so I convert it to `0` for male and `1` for female.\n",
        "\n",
        "Some ages are empty. I replace them with the mean age so I do not throw away data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def parse_float(value):\n",
        "    return float(value) if value else math.nan\n",
        "\n",
        "ages = [parse_float(r['Age']) for r in rows if r['Age']]\n",
        "fares = [parse_float(r['Fare']) for r in rows if r['Fare']]\n",
        "mean_age = statistics.mean(ages)\n",
        "mean_fare = statistics.mean(fares)\n",
        "\n",
        "\n",
        "def preprocess_row(row):\n",
        "    age = parse_float(row['Age'])\n",
        "    fare = parse_float(row['Fare'])\n",
        "    if math.isnan(age):\n",
        "        age = mean_age\n",
        "    if math.isnan(fare):\n",
        "        fare = mean_fare\n",
        "    features = [\n",
        "        age,\n",
        "        float(row['SibSp']),\n",
        "        float(row['Parch']),\n",
        "        fare,\n",
        "        1.0 if row['Sex'] == 'female' else 0.0,\n",
        "    ]\n",
        "    return {\n",
        "        'features': features,\n",
        "        'label': int(row['Survived']),\n",
        "        'raw': row,\n",
        "    }\n",
        "\n",
        "processed = [preprocess_row(r) for r in rows]\n",
        "processed[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Train / test split helper\n",
        "\n",
        "We follow the class rule and shuffle with seed 90 before slicing the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_test_split(items, test_ratio=0.3):\n",
        "    indexes = list(range(len(items)))\n",
        "    random.Random(90).shuffle(indexes)\n",
        "    split = int(len(items) * (1 - test_ratio))\n",
        "    train_idx = indexes[:split]\n",
        "    test_idx = indexes[split:]\n",
        "    train = [items[i] for i in train_idx]\n",
        "    test = [items[i] for i in test_idx]\n",
        "    return train, test\n",
        "\n",
        "train_data, test_data = train_test_split(processed)\n",
        "len(train_data), len(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Metrics and confusion matrix\n",
        "\n",
        "Accuracy is enough for now. I also coded the confusion matrix so the rows represent the predicted label (teacher reminder: Python prints the matrix flipped if we do not do this by hand).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    correct = sum(int(a == b) for a, b in zip(y_true, y_pred))\n",
        "    return correct / len(y_true)\n",
        "\n",
        "\n",
        "def confusion_matrix(y_true, y_pred):\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "    table = {pred: {actual: 0 for actual in labels} for pred in labels}\n",
        "    for actual, predicted in zip(y_true, y_pred):\n",
        "        table[predicted][actual] += 1\n",
        "    return table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ZeroR baseline\n",
        "\n",
        "ZeroR just predicts the most frequent class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_zero_r(dataset):\n",
        "    counter = Counter(item['label'] for item in dataset)\n",
        "    majority_label, _ = counter.most_common(1)[0]\n",
        "    return majority_label\n",
        "\n",
        "\n",
        "def predict_zero_r(model, dataset):\n",
        "    return [model for _ in dataset]\n",
        "\n",
        "zero_r_model = train_zero_r(train_data)\n",
        "zero_r_predictions = predict_zero_r(zero_r_model, test_data)\n",
        "zero_r_acc = accuracy([d['label'] for d in test_data], zero_r_predictions)\n",
        "zero_r_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. OneR rule\n",
        "\n",
        "I try every single column and pick the one with the smallest error. For numeric columns I create four equal-width buckets so the rule stays readable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def make_bins(values, bins=4):\n",
        "    clean = sorted(v for v in values if not math.isnan(v))\n",
        "    if not clean:\n",
        "        return []\n",
        "    step = len(clean) // bins\n",
        "    if step == 0:\n",
        "        return sorted(set(clean))\n",
        "    cuts = []\n",
        "    for i in range(1, bins):\n",
        "        cuts.append(clean[min(i * step, len(clean) - 1)])\n",
        "    return sorted(set(cuts))\n",
        "\n",
        "\n",
        "numeric_columns = {\n",
        "    'Age': [parse_float(r['Age']) if r['Age'] else math.nan for r in rows],\n",
        "    'Fare': [parse_float(r['Fare']) if r['Fare'] else math.nan for r in rows],\n",
        "}\n",
        "\n",
        "bin_rules = {name: make_bins(vals) for name, vals in numeric_columns.items()}\n",
        "\n",
        "\n",
        "def bucketize(value, cuts):\n",
        "    if math.isnan(value):\n",
        "        return 'missing'\n",
        "    for threshold in cuts:\n",
        "        if value <= threshold:\n",
        "            return f\"<= {threshold:.2f}\"\n",
        "    return '> last'\n",
        "\n",
        "\n",
        "def train_one_r(dataset):\n",
        "    candidates = ['Sex', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "    best_feature = None\n",
        "    best_error = float('inf')\n",
        "    best_rule = None\n",
        "\n",
        "    for feature in candidates:\n",
        "        table = defaultdict(Counter)\n",
        "        for item in dataset:\n",
        "            row = item['raw']\n",
        "            label = item['label']\n",
        "            if feature in ('Age', 'Fare'):\n",
        "                value = bucketize(parse_float(row[feature]) if row[feature] else math.nan, bin_rules[feature])\n",
        "            else:\n",
        "                value = row[feature]\n",
        "            table[value][label] += 1\n",
        "\n",
        "        rules = {}\n",
        "        error = 0\n",
        "        for value, counts in table.items():\n",
        "            chosen_label, chosen_count = counts.most_common(1)[0]\n",
        "            rules[value] = chosen_label\n",
        "            error += sum(counts.values()) - chosen_count\n",
        "\n",
        "        if error < best_error:\n",
        "            best_error = error\n",
        "            best_feature = feature\n",
        "            best_rule = dict(rules)\n",
        "\n",
        "    return {'feature': best_feature, 'rules': best_rule}\n",
        "\n",
        "\n",
        "def predict_one_r(model, dataset):\n",
        "    feature = model['feature']\n",
        "    rules = model['rules']\n",
        "    predictions = []\n",
        "    for item in dataset:\n",
        "        row = item['raw']\n",
        "        if feature in ('Age', 'Fare'):\n",
        "            value = bucketize(parse_float(row[feature]) if row[feature] else math.nan, bin_rules[feature])\n",
        "        else:\n",
        "            value = row[feature]\n",
        "        predictions.append(rules.get(value, 0))\n",
        "    return predictions\n",
        "\n",
        "one_r_model = train_one_r(train_data)\n",
        "one_r_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rule tells me which single column worked best. Now I check the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "one_r_predictions = predict_one_r(one_r_model, test_data)\n",
        "one_r_acc = accuracy([d['label'] for d in test_data], one_r_predictions)\n",
        "one_r_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tiny k-NN (k = 3)\n",
        "\n",
        "I only use the numeric features we cleaned above.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def euclidean_distance(a, b):\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "\n",
        "def predict_knn(train_set, test_set, k=3):\n",
        "    predictions = []\n",
        "    for test_item in test_set:\n",
        "        distances = []\n",
        "        for train_item in train_set:\n",
        "            dist = euclidean_distance(test_item['features'], train_item['features'])\n",
        "            distances.append((dist, train_item['label']))\n",
        "        distances.sort(key=lambda x: x[0])\n",
        "        nearest = distances[:k]\n",
        "        vote = Counter(label for _, label in nearest).most_common(1)[0][0]\n",
        "        predictions.append(vote)\n",
        "    return predictions\n",
        "\n",
        "knn_predictions = predict_knn(train_data, test_data, k=3)\n",
        "knn_acc = accuracy([d['label'] for d in test_data], knn_predictions)\n",
        "knn_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Confusion matrix for the best model\n",
        "\n",
        "k-NN was slightly better on my split, so I print its confusion matrix with the row = predicted, column = actual format the professor mentioned.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "knn_confusion = confusion_matrix([d['label'] for d in test_data], knn_predictions)\n",
        "knn_confusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Simple 3-fold cross-validation\n",
        "\n",
        "I still stay in pure Python. I manually rotate three folds so we get a tiny taste of validation without fancy libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def k_fold_split(dataset, k=3):\n",
        "    indexes = list(range(len(dataset)))\n",
        "    random.Random(90).shuffle(indexes)\n",
        "    fold_size = len(dataset) // k\n",
        "    folds = []\n",
        "    for i in range(k):\n",
        "        start = i * fold_size\n",
        "        end = start + fold_size\n",
        "        folds.append(indexes[start:end])\n",
        "    leftovers = indexes[k * fold_size:]\n",
        "    for idx, extra in enumerate(leftovers):\n",
        "        folds[idx % k].append(extra)\n",
        "    return folds\n",
        "\n",
        "\n",
        "def cross_validate(dataset, predict_func, k=3):\n",
        "    folds = k_fold_split(dataset, k)\n",
        "    scores = []\n",
        "    for i in range(k):\n",
        "        test_idx = folds[i]\n",
        "        train_idx = [idx for j, fold in enumerate(folds) if j != i for idx in fold]\n",
        "        train_set = [dataset[idx] for idx in train_idx]\n",
        "        test_set = [dataset[idx] for idx in test_idx]\n",
        "        predictions = predict_func(train_set, test_set)\n",
        "        score = accuracy([item['label'] for item in test_set], predictions)\n",
        "        scores.append(score)\n",
        "    return scores\n",
        "\n",
        "zero_r_cv = cross_validate(processed, lambda train, test: predict_zero_r(train_zero_r(train), test))\n",
        "one_r_cv = cross_validate(processed, lambda train, test: predict_one_r(train_one_r(train), test))\n",
        "knn_cv = cross_validate(processed, lambda train, test: predict_knn(train, test, k=3))\n",
        "\n",
        "zero_r_cv, one_r_cv, knn_cv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I also compute the mean accuracy of each list to summarize the table.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cv_summary = {\n",
        "    'ZeroR': statistics.mean(zero_r_cv),\n",
        "    'OneR': statistics.mean(one_r_cv),\n",
        "    'kNN (k=3)': statistics.mean(knn_cv),\n",
        "}\n",
        "cv_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Final thoughts\n",
        "\n",
        "* ZeroR is the baseline. It helped me see that the dataset is imbalanced.\n",
        "* OneR quickly showed that `Sex` is the strongest single feature.\n",
        "* k-NN gave the best accuracy when I used four numeric columns plus the simple sex flag.\n",
        "\n",
        "This matches what we talked about in class: start simple, keep the math readable, and stick to the fixed random seed so everyone gets the same answers.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}