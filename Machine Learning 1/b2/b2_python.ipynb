{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# B2 Python practice \u2013 novice walk-through\n",
        "\n",
        "I follow the `B2_python.pdf` bulletin using the Titanic `train.csv`. I keep every step tiny and simple so a beginner can re-run it without guessing hidden tricks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load the Titanic `train.csv`\n",
        "\n",
        "The professor insisted on using the `train.csv` sheet. I copy it inside the `b2` folder and let the notebook search for it starting from the current working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import math\n",
        "import random\n",
        "import statistics\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "from typing import Iterable\n",
        "\n",
        "random.seed(90)  # class seed\n",
        "\n",
        "search_dirs = [Path.cwd()] + list(Path.cwd().parents)\n",
        "csv_path = None\n",
        "for directory in search_dirs:\n",
        "    candidate = directory / \"Machine Learning 1\" / \"b2\" / \"train.csv\"\n",
        "    if candidate.exists():\n",
        "        csv_path = candidate\n",
        "        break\n",
        "\n",
        "if csv_path is None:\n",
        "    raise FileNotFoundError(\"Could not find Machine Learning 1/b2/train.csv near this notebook.\")\n",
        "\n",
        "rows: list[dict[str, str]] = []\n",
        "with csv_path.open(encoding=\"utf-8\") as handle:\n",
        "    reader = csv.DictReader(handle)\n",
        "    for row in reader:\n",
        "        rows.append(row)\n",
        "\n",
        "(len(rows), list(rows[0].keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I peek at a couple of rows to make sure the characters look fine and that the file really matches the Titanic training sheet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for sample in rows[:5]:\n",
        "    print(sample[\"PassengerId\"], sample[\"Name\"], sample[\"Sex\"], sample[\"Survived\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.1 Quick dataset checks\n",
        "\n",
        "A quick size and missing-value summary keeps me aligned with the theory section about data understanding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "passenger_count = len(rows)\n",
        "columns = list(rows[0].keys())\n",
        "print(f\"Passengers: {passenger_count}\")\n",
        "print(f\"Columns: {columns}\")\n",
        "\n",
        "missing_by_column = {\n",
        "    column: sum(1 for row in rows if not row[column]) for column in columns\n",
        "}\n",
        "missing_by_column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Survival balance\n",
        "\n",
        "Before training any model I check how many people survived. Zero means the passenger did not survive and one means the opposite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_counts = Counter(int(r[\"Survived\"]) for r in rows)\n",
        "survival_rate = label_counts[1] / passenger_count\n",
        "print(\"Label counts:\", label_counts)\n",
        "print(f\"Survival rate: {survival_rate:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare helper functions\n",
        "\n",
        "The bulletin requests tiny implementations of ZeroR, OneR, and k-NN. I implement them here step by step so the code stays visible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Basic parsing and defaults\n",
        "\n",
        "Some values are missing, especially ages and the port of embarkation. I reuse the average or the most common value so no row disappears."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_float(value: str) -> float:\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (TypeError, ValueError):\n",
        "        return math.nan\n",
        "\n",
        "age_values = [parse_float(row[\"Age\"]) for row in rows if row[\"Age\"]]\n",
        "fare_values = [parse_float(row[\"Fare\"]) for row in rows if row[\"Fare\"]]\n",
        "mean_age = statistics.mean(age_values)\n",
        "median_fare = statistics.median(fare_values)\n",
        "most_common_embarked = Counter(row[\"Embarked\"] or \"S\" for row in rows).most_common(1)[0][0]\n",
        "\n",
        "mean_age, median_fare, most_common_embarked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Turn rows into numeric features\n",
        "\n",
        "k-NN needs numbers, so I turn each passenger into a simple feature vector. I keep the raw values too so OneR can look at the original columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def encode_embarked(value: str) -> float:\n",
        "    mapping = {\"S\": 0.0, \"C\": 1.0, \"Q\": 2.0}\n",
        "    clean = (value or most_common_embarked).strip().upper()\n",
        "    return mapping.get(clean, mapping[most_common_embarked])\n",
        "\n",
        "\n",
        "def preprocess_row(row: dict[str, str]) -> dict[str, object]:\n",
        "    age = parse_float(row[\"Age\"])\n",
        "    if math.isnan(age):\n",
        "        age = mean_age\n",
        "    fare = parse_float(row[\"Fare\"])\n",
        "    if math.isnan(fare):\n",
        "        fare = median_fare\n",
        "    sibsp = int(row[\"SibSp\"])\n",
        "    parch = int(row[\"Parch\"])\n",
        "    pclass = int(row[\"Pclass\"])\n",
        "    sex_clean = row[\"Sex\"].strip().lower()\n",
        "    sex_flag = 1.0 if sex_clean == \"female\" else 0.0\n",
        "    embarked_clean = (row[\"Embarked\"] or most_common_embarked).strip().upper()\n",
        "    embarked_flag = encode_embarked(embarked_clean)\n",
        "    features_raw = [\n",
        "        float(age),\n",
        "        float(sibsp),\n",
        "        float(parch),\n",
        "        float(fare),\n",
        "        sex_flag,\n",
        "        float(pclass),\n",
        "        embarked_flag,\n",
        "    ]\n",
        "    return {\n",
        "        \"label\": int(row[\"Survived\"]),\n",
        "        \"age\": float(age),\n",
        "        \"sibsp\": sibsp,\n",
        "        \"parch\": parch,\n",
        "        \"fare\": float(fare),\n",
        "        \"pclass\": pclass,\n",
        "        \"sex\": sex_clean,\n",
        "        \"embarked\": embarked_clean,\n",
        "        \"sex_flag\": sex_flag,\n",
        "        \"embarked_flag\": embarked_flag,\n",
        "        \"features_raw\": features_raw,\n",
        "        \"passenger_id\": int(row[\"PassengerId\"]),\n",
        "        \"name\": row[\"Name\"],\n",
        "    }\n",
        "\n",
        "processed_rows = [preprocess_row(row) for row in rows]\n",
        "processed_rows[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Standardise numeric features\n",
        "\n",
        "I reuse the training averages and standard deviations so every feature stays on a similar scale before running k-NN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_standard_scaler(dataset: list[dict[str, object]]) -> tuple[list[float], list[float]]:\n",
        "    columns = len(dataset[0][\"features_raw\"])\n",
        "    means: list[float] = []\n",
        "    stds: list[float] = []\n",
        "    for col in range(columns):\n",
        "        values = [row[\"features_raw\"][col] for row in dataset]\n",
        "        mean = statistics.mean(values)\n",
        "        std = statistics.pstdev(values)\n",
        "        if std == 0:\n",
        "            std = 1.0\n",
        "        means.append(mean)\n",
        "        stds.append(std)\n",
        "    return means, stds\n",
        "\n",
        "\n",
        "def scale_row(row: dict[str, object], means: list[float], stds: list[float]) -> dict[str, object]:\n",
        "    scaled = []\n",
        "    for value, mean, std in zip(row[\"features_raw\"], means, stds):\n",
        "        scaled.append((value - mean) / std if std else 0.0)\n",
        "    new_row = dict(row)\n",
        "    new_row[\"features\"] = scaled\n",
        "    return new_row\n",
        "\n",
        "\n",
        "def scale_dataset(dataset: list[dict[str, object]], means: list[float], stds: list[float]) -> list[dict[str, object]]:\n",
        "    return [scale_row(row, means, stds) for row in dataset]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Train/test split helper\n",
        "\n",
        "The bulletin mentions a classic hold-out evaluation. I shuffle once with the shared seed (90) and reuse these helpers in later sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scale_with_training(\n",
        "    train_rows: list[dict[str, object]],\n",
        "    test_rows: list[dict[str, object]],\n",
        ") -> tuple[list[dict[str, object]], list[dict[str, object]], tuple[list[float], list[float]]]:\n",
        "    means, stds = fit_standard_scaler(train_rows)\n",
        "    return scale_dataset(train_rows, means, stds), scale_dataset(test_rows, means, stds), (means, stds)\n",
        "\n",
        "\n",
        "def prepare_split(\n",
        "    dataset: list[dict[str, object]],\n",
        "    test_ratio: float = 0.3,\n",
        "    seed: int = 90,\n",
        ") -> tuple[list[dict[str, object]], list[dict[str, object]], tuple[list[float], list[float]]]:\n",
        "    indexes = list(range(len(dataset)))\n",
        "    random.Random(seed).shuffle(indexes)\n",
        "    split_point = int(len(indexes) * (1 - test_ratio))\n",
        "    train_idx = indexes[:split_point]\n",
        "    test_idx = indexes[split_point:]\n",
        "    train_rows = [dataset[i] for i in train_idx]\n",
        "    test_rows = [dataset[i] for i in test_idx]\n",
        "    return scale_with_training(train_rows, test_rows)\n",
        "\n",
        "\n",
        "train_data, test_data, scaler = prepare_split(processed_rows)\n",
        "len(train_data), len(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Metrics and evaluation helpers\n",
        "\n",
        "Accuracy and a confusion matrix (rows = predictions, columns = actual labels) are enough for this bulletin."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(y_true: list[int], y_pred: list[int]) -> float:\n",
        "    correct = sum(1 for actual, predicted in zip(y_true, y_pred) if actual == predicted)\n",
        "    return correct / len(y_true)\n",
        "\n",
        "\n",
        "def confusion_matrix(y_true: list[int], y_pred: list[int]) -> dict[int, dict[int, int]]:\n",
        "    labels = sorted(set(y_true) | set(y_pred))\n",
        "    table: dict[int, dict[int, int]] = {pred: {actual: 0 for actual in labels} for pred in labels}\n",
        "    for actual, predicted in zip(y_true, y_pred):\n",
        "        table[predicted][actual] += 1\n",
        "    return table\n",
        "\n",
        "\n",
        "def print_confusion(table: dict[int, dict[int, int]]) -> None:\n",
        "    labels = sorted(next(iter(table.values())).keys())\n",
        "    header = \"predicted -> actual\".ljust(16) + \" \".join(str(label).center(8) for label in labels)\n",
        "    print(header)\n",
        "    for pred_label in sorted(table.keys()):\n",
        "        row = str(pred_label).center(16)\n",
        "        for actual_label in labels:\n",
        "            row += str(table[pred_label][actual_label]).center(8)\n",
        "        print(row)\n",
        "\n",
        "\n",
        "def evaluate_models(\n",
        "    train_split: list[dict[str, object]],\n",
        "    test_split: list[dict[str, object]],\n",
        ") -> dict[str, float]:\n",
        "    y_true = [row[\"label\"] for row in test_split]\n",
        "    zero_r_model = train_zero_r(train_split)\n",
        "    zero_r_predictions = predict_zero_r(zero_r_model, test_split)\n",
        "    one_r_model = train_one_r(train_split)\n",
        "    one_r_predictions = predict_one_r(one_r_model, test_split)\n",
        "    knn_predictions = predict_knn(train_split, test_split, k=3)\n",
        "    return {\n",
        "        \"ZeroR\": accuracy(y_true, zero_r_predictions),\n",
        "        \"OneR\": accuracy(y_true, one_r_predictions),\n",
        "        \"kNN (k=3)\": accuracy(y_true, knn_predictions),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. ZeroR baseline\n",
        "\n",
        "ZeroR always predicts the most common class in the training data. It gives me a lower bound for accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_zero_r(dataset: list[dict[str, object]]) -> int:\n",
        "    labels = Counter(row[\"label\"] for row in dataset)\n",
        "    return labels.most_common(1)[0][0]\n",
        "\n",
        "\n",
        "def predict_zero_r(model: int, dataset: list[dict[str, object]]) -> list[int]:\n",
        "    return [model for _ in dataset]\n",
        "\n",
        "\n",
        "zero_r_model = train_zero_r(train_data)\n",
        "zero_r_predictions = predict_zero_r(zero_r_model, test_data)\n",
        "zero_r_acc = accuracy([row[\"label\"] for row in test_data], zero_r_predictions)\n",
        "zero_r_model, zero_r_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. OneR rule\n",
        "\n",
        "OneR searches for the single column that delivers the smallest error. Numeric values are discretised into four equal-width bins to keep the explanations easy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_bins(values: list[float], bins: int = 4) -> list[float]:\n",
        "    clean = sorted(v for v in values if not math.isnan(v))\n",
        "    if not clean:\n",
        "        return []\n",
        "    step = max(1, len(clean) // bins)\n",
        "    cuts = []\n",
        "    for i in range(1, bins):\n",
        "        cuts.append(clean[min(i * step, len(clean) - 1)])\n",
        "    return sorted(set(cuts))\n",
        "\n",
        "\n",
        "def assign_bin(value: float, cuts: list[float]) -> str:\n",
        "    if not cuts:\n",
        "        return \"all\"\n",
        "    if math.isnan(value):\n",
        "        return \"missing\"\n",
        "    for cut in cuts:\n",
        "        if value <= cut:\n",
        "            return f\"<= {cut:.2f}\"\n",
        "    return f\"> {cuts[-1]:.2f}\"\n",
        "\n",
        "\n",
        "def train_one_r(dataset: list[dict[str, object]]) -> dict[str, object]:\n",
        "    candidate_features = [\n",
        "        \"sex\",\n",
        "        \"pclass\",\n",
        "        \"embarked\",\n",
        "        \"age\",\n",
        "        \"sibsp\",\n",
        "        \"parch\",\n",
        "        \"fare\",\n",
        "    ]\n",
        "    best_feature = None\n",
        "    best_rules: dict[str, int] | dict[int, int] | None = None\n",
        "    best_cuts: list[float] | None = None\n",
        "    best_error = float(\"inf\")\n",
        "    default_label = train_zero_r(dataset)\n",
        "\n",
        "    for feature in candidate_features:\n",
        "        if feature in {\"sex\", \"embarked\"}:\n",
        "            groups: dict[str, Counter[int]] = defaultdict(Counter)\n",
        "            for row in dataset:\n",
        "                groups[str(row[feature])][row[\"label\"]] += 1\n",
        "            rules = {value: counter.most_common(1)[0][0] for value, counter in groups.items()}\n",
        "            errors = sum(1 for row in dataset if rules.get(str(row[feature]), default_label) != row[\"label\"])\n",
        "            if errors < best_error:\n",
        "                best_feature = feature\n",
        "                best_rules = rules\n",
        "                best_cuts = None\n",
        "                best_error = errors\n",
        "        else:\n",
        "            values = [float(row[feature]) for row in dataset]\n",
        "            cuts = make_bins(values)\n",
        "            groups: dict[str, Counter[int]] = defaultdict(Counter)\n",
        "            for row in dataset:\n",
        "                bucket = assign_bin(float(row[feature]), cuts)\n",
        "                groups[bucket][row[\"label\"]] += 1\n",
        "            rules = {bucket: counter.most_common(1)[0][0] for bucket, counter in groups.items()}\n",
        "            errors = sum(1 for row in dataset if rules.get(assign_bin(float(row[feature]), cuts), default_label) != row[\"label\"])\n",
        "            if errors < best_error:\n",
        "                best_feature = feature\n",
        "                best_rules = rules\n",
        "                best_cuts = cuts\n",
        "                best_error = errors\n",
        "\n",
        "    return {\n",
        "        \"feature\": best_feature,\n",
        "        \"rules\": best_rules or {},\n",
        "        \"cuts\": best_cuts,\n",
        "        \"default\": default_label,\n",
        "    }\n",
        "\n",
        "\n",
        "def predict_one_r(model: dict[str, object], dataset: list[dict[str, object]]) -> list[int]:\n",
        "    feature = model[\"feature\"]\n",
        "    rules = model[\"rules\"]\n",
        "    cuts = model[\"cuts\"]\n",
        "    default_label = model[\"default\"]\n",
        "    predictions: list[int] = []\n",
        "    for row in dataset:\n",
        "        if cuts is None:\n",
        "            key = str(row[feature])\n",
        "        else:\n",
        "            key = assign_bin(float(row[feature]), cuts)\n",
        "        predictions.append(int(rules.get(key, default_label)))\n",
        "    return predictions\n",
        "\n",
        "\n",
        "one_r_model = train_one_r(train_data)\n",
        "one_r_predictions = predict_one_r(one_r_model, test_data)\n",
        "one_r_acc = accuracy([row[\"label\"] for row in test_data], one_r_predictions)\n",
        "one_r_model, one_r_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Tiny k-NN (k = 3)\n",
        "\n",
        "With scaled numeric features I can compute Euclidean distances by hand. Three neighbours keep the voting rule short."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def euclidean_distance(a: list[float], b: list[float]) -> float:\n",
        "    return math.sqrt(sum((x - y) ** 2 for x, y in zip(a, b)))\n",
        "\n",
        "\n",
        "def predict_knn(\n",
        "    train_set: list[dict[str, object]],\n",
        "    test_set: list[dict[str, object]],\n",
        "    k: int = 3,\n",
        ") -> list[int]:\n",
        "    predictions: list[int] = []\n",
        "    for test_item in test_set:\n",
        "        distances: list[tuple[float, int]] = []\n",
        "        for train_item in train_set:\n",
        "            dist = euclidean_distance(test_item[\"features\"], train_item[\"features\"])\n",
        "            distances.append((dist, train_item[\"label\"]))\n",
        "        distances.sort(key=lambda pair: pair[0])\n",
        "        neighbours = distances[:k]\n",
        "        vote = Counter(label for _, label in neighbours).most_common(1)[0][0]\n",
        "        predictions.append(vote)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "knn_predictions = predict_knn(train_data, test_data, k=3)\n",
        "knn_acc = accuracy([row[\"label\"] for row in test_data], knn_predictions)\n",
        "knn_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hold-out summary and confusion matrix\n",
        "\n",
        "I compare the three algorithms on the same split and then print the confusion matrix of the best one (rows = predictions, columns = actual labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "holdout_scores = {\n",
        "    \"ZeroR\": zero_r_acc,\n",
        "    \"OneR\": one_r_acc,\n",
        "    \"kNN (k=3)\": knn_acc,\n",
        "}\n",
        "holdout_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model_name = max(holdout_scores, key=holdout_scores.get)\n",
        "print(\"Best model:\", best_model_name)\n",
        "if best_model_name == \"ZeroR\":\n",
        "    best_predictions = zero_r_predictions\n",
        "elif best_model_name == \"OneR\":\n",
        "    best_predictions = one_r_predictions\n",
        "else:\n",
        "    best_predictions = knn_predictions\n",
        "\n",
        "matrix = confusion_matrix([row[\"label\"] for row in test_data], best_predictions)\n",
        "print_confusion(matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Repeated hold-out (5 runs)\n",
        "\n",
        "To reduce the luck of a single shuffle I repeat the hold-out evaluation five times, always building the scaler from the training portion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def repeated_holdout(\n",
        "    dataset: list[dict[str, object]],\n",
        "    repeats: int = 5,\n",
        "    test_ratio: float = 0.3,\n",
        "    start_seed: int = 90,\n",
        ") -> dict[str, list[float]]:\n",
        "    scores = {\"ZeroR\": [], \"OneR\": [], \"kNN (k=3)\": []}\n",
        "    for offset in range(repeats):\n",
        "        seed = start_seed + offset\n",
        "        train_split, test_split, _ = prepare_split(dataset, test_ratio=test_ratio, seed=seed)\n",
        "        result = evaluate_models(train_split, test_split)\n",
        "        for name, score in result.items():\n",
        "            scores[name].append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "repeated_scores = repeated_holdout(processed_rows, repeats=5)\n",
        "repeated_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean(values: Iterable[float]) -> float:\n",
        "    values = list(values)\n",
        "    return sum(values) / len(values)\n",
        "\n",
        "\n",
        "repeated_summary = {name: mean(scores) for name, scores in repeated_scores.items()}\n",
        "repeated_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Manual 3-fold cross-validation\n",
        "\n",
        "Following the class slides, I rotate three folds by hand. Each fold rebuilds the scaler from the current training data before predicting the validation fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def k_fold_validation(\n",
        "    dataset: list[dict[str, object]],\n",
        "    k: int = 3,\n",
        "    seed: int = 90,\n",
        ") -> dict[str, list[float]]:\n",
        "    indexes = list(range(len(dataset)))\n",
        "    random.Random(seed).shuffle(indexes)\n",
        "    fold_sizes = [len(dataset) // k for _ in range(k)]\n",
        "    for i in range(len(dataset) % k):\n",
        "        fold_sizes[i] += 1\n",
        "    folds: list[list[int]] = []\n",
        "    start = 0\n",
        "    for size in fold_sizes:\n",
        "        folds.append(indexes[start:start + size])\n",
        "        start += size\n",
        "\n",
        "    scores = {\"ZeroR\": [], \"OneR\": [], \"kNN (k=3)\": []}\n",
        "    for fold_index in range(k):\n",
        "        test_idx = set(folds[fold_index])\n",
        "        train_rows = [dataset[i] for i in indexes if i not in test_idx]\n",
        "        test_rows = [dataset[i] for i in indexes if i in test_idx]\n",
        "        train_split, test_split, _ = scale_with_training(train_rows, test_rows)\n",
        "        result = evaluate_models(train_split, test_split)\n",
        "        for name, score in result.items():\n",
        "            scores[name].append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "cv_scores = k_fold_validation(processed_rows, k=3)\n",
        "cv_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv_summary = {name: mean(scores) for name, scores in cv_scores.items()}\n",
        "cv_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. OneR rule on the full dataset\n",
        "\n",
        "After validating the ideas I retrain OneR on every passenger to keep a final rule ready for the exam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "full_one_r_model = train_one_r(processed_rows)\n",
        "full_one_r_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Manual k-NN prediction example\n",
        "\n",
        "The bulletin also asks for a manual k-NN check. I create a pretend passenger and show the three closest neighbours together with the final vote."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_means, all_stds = fit_standard_scaler(processed_rows)\n",
        "full_scaled = scale_dataset(processed_rows, all_means, all_stds)\n",
        "\n",
        "manual_raw = {\n",
        "    \"PassengerId\": \"0\",\n",
        "    \"Survived\": \"0\",  # placeholder\n",
        "    \"Pclass\": \"2\",\n",
        "    \"Name\": \"Manual Passenger\",\n",
        "    \"Sex\": \"female\",\n",
        "    \"Age\": \"29\",\n",
        "    \"SibSp\": \"0\",\n",
        "    \"Parch\": \"0\",\n",
        "    \"Ticket\": \"\",\n",
        "    \"Fare\": \"23.45\",\n",
        "    \"Cabin\": \"\",\n",
        "    \"Embarked\": \"S\",\n",
        "}\n",
        "manual_processed = preprocess_row(manual_raw)\n",
        "manual_scaled = scale_row(manual_processed, all_means, all_stds)\n",
        "\n",
        "neighbour_distances = []\n",
        "for row in full_scaled:\n",
        "    dist = euclidean_distance(manual_scaled[\"features\"], row[\"features\"])\n",
        "    neighbour_distances.append((dist, row))\n",
        "\n",
        "neighbour_distances.sort(key=lambda pair: pair[0])\n",
        "closest_three = neighbour_distances[:3]\n",
        "for distance, neighbour in closest_three:\n",
        "    print(\n",
        "        f\"Distance: {distance:.3f} | Survived: {neighbour['label']} | \"\n",
        "        f\"Sex: {neighbour['sex']} | Age: {neighbour['age']} | Fare: {neighbour['fare']}\"\n",
        "    )\n",
        "\n",
        "manual_prediction = predict_knn(full_scaled, [manual_scaled], k=3)[0]\n",
        "manual_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final thoughts\n",
        "\n",
        "* ZeroR provides the reference accuracy and reminds me that most passengers did not survive.\n",
        "* OneR usually picks the `sex` column, matching the class notes about simple yet strong rules.\n",
        "* k-NN benefits from scaling and tends to achieve the best accuracy on both hold-out and cross-validation.\n",
        "* The manual example confirms how to run the neighbour vote without external libraries.\n",
        "\n",
        "Everything stays within the standard library and respects the shared random seed of 90."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}