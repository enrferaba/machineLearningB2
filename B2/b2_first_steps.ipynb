{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing a tiny supervised learning flow (B2 level)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am following the first supervised learning steps from the \"Machine Learning 1\" notes. My goal is to keep the code ",
    "simple and to explain every move like a beginner who is still connecting the dots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will:\n",
    "1. Load the small Iris CSV file that was shared with the class.\n",
    "2. Explore basic statistics to imitate the descriptive analysis suggested in the theory.\n",
    "3. Split the data into train and test parts so I can evaluate honestly.\n",
    "4. Build a tiny k-nearest neighbors (k-NN) classifier from scratch using only Python basics.\n",
    "5. Measure the accuracy and try a manual prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "possible_paths = [\n",
    "    Path(\"iris_sample.csv\"),\n",
    "    Path(\"B2/iris_sample.csv\"),\n",
    "]\n",
    "\n",
    "for option in possible_paths:\n",
    "    if option.exists():\n",
    "        data_path = option\n",
    "        break\n",
    "else:\n",
    "    raise FileNotFoundError(\"Could not find iris_sample.csv next to the notebook.\")\n",
    "\n",
    "print(f\"Using data file: {data_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a friendly mini data set\n",
    "The notes talk about the Iris flowers, so I read the provided CSV file (four features + species).\n",
    "Having the values on disk keeps the project self contained and still feels like a real workflow."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "with data_path.open(newline=\"\") as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    iris_rows = [row for row in reader]\n",
    "    header = reader.fieldnames or []\n",
    "\n",
    "if not iris_rows:\n",
    "    raise ValueError(\"The CSV file is empty.\")\n",
    "\n",
    "print(f\"Columns found: {header}\")\n",
    "print(f\"Loaded {len(iris_rows)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the rows into numbers and labels\n",
    "The theory says we work with feature vectors and targets.\n",
    "I convert each line into a tuple: ([features], label)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_columns = [name for name in header if name != \"species\"]\n",
    "\n",
    "def parse_rows(rows):\n",
    "    dataset = []\n",
    "    for row in rows:\n",
    "        features = [float(row[column]) for column in feature_columns]\n",
    "        label = row[\"species\"]\n",
    "        dataset.append((features, label))\n",
    "    return dataset\n",
    "\n",
    "full_dataset = parse_rows(iris_rows)\n",
    "print(full_dataset[0])\n",
    "print(f\"Total samples: {len(full_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the basic statistics\n",
    "To keep it simple I calculate minimum, maximum, and average for each feature.\n",
    "This mimics the descriptive analysis chapter."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def describe_feature(dataset, index):\n",
    "    values = [row[0][index] for row in dataset]\n",
    "    minimum = min(values)\n",
    "    maximum = max(values)\n",
    "    average = sum(values) / len(values)\n",
    "    return minimum, maximum, average\n",
    "\n",
    "friendly_feature_names = [\n",
    "    \"sepal length (cm)\",\n",
    "    \"sepal width (cm)\",\n",
    "    \"petal length (cm)\",\n",
    "    \"petal width (cm)\",\n",
    "]\n",
    "\n",
    "for idx, friendly_name in enumerate(friendly_feature_names):\n",
    "    min_val, max_val, avg_val = describe_feature(full_dataset, idx)\n",
    "    print(f\"{friendly_name}: min={min_val:.1f}, max={max_val:.1f}, avg={avg_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "I shuffle the data with the fixed seed that our teacher mentioned (90) so the results stay repeatable.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_test_split(dataset, test_ratio=0.25, seed=90):\n",
    "    random.seed(seed)\n",
    "    shuffled = dataset[:]\n",
    "    random.shuffle(shuffled)\n",
    "    test_size = max(1, int(len(shuffled) * test_ratio))\n",
    "    test_data = shuffled[:test_size]\n",
    "    train_data = shuffled[test_size:]\n",
    "    return train_data, test_data\n",
    "\n",
    "train_data, test_data = train_test_split(full_dataset)\n",
    "print(f\"Train size: {len(train_data)}\")\n",
    "print(f\"Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a pocket k-NN classifier\n",
    "The theory explains that k-NN looks at the closest examples.\n",
    "I use the Euclidean distance and keep k = 3 neighbors because it is a small data set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def euclidean_distance(a, b):\n    total = 0.0\n    for value_a, value_b in zip(a, b):\n        total += (value_a - value_b) ** 2\n    return total ** 0.5\n\n\ndef knn_predict(train_set, new_sample, k=3):\n    distances = []\n    for features, label in train_set:\n        distance = euclidean_distance(features, new_sample)\n        distances.append((distance, label))\n    distances.sort(key=lambda item: item[0])\n    neighbors = distances[:k]\n    votes = {}\n    for _, neighbor_label in neighbors:\n        votes[neighbor_label] = votes.get(neighbor_label, 0) + 1\n    best_label = max(votes.items(), key=lambda item: item[1])[0]\n    return best_label\n\nprint(knn_predict(train_data, test_data[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating accuracy\n",
    "I check how many test samples the classifier gets right and compute the proportion."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def accuracy_score(model_data, test_set, k=3):\n",
    "    correct = 0\n",
    "    evaluation_pairs = []\n",
    "    for features, expected_label in test_set:\n",
    "        predicted_label = knn_predict(model_data, features, k=k)\n",
    "        evaluation_pairs.append((expected_label, predicted_label))\n",
    "        if predicted_label == expected_label:\n",
    "            correct += 1\n",
    "    accuracy_value = correct / len(test_set)\n",
    "    return accuracy_value, evaluation_pairs\n",
    "\n",
    "accuracy, evaluation_pairs = accuracy_score(train_data, test_data)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix with the classroom orientation\n",
    "\n",
    "Our professor warned us that the confusion matrix in Python is often shown the other way around, so I build it with predicted labels on the rows and actual labels on the columns to match that note.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_confusion_matrix(pairs):\n",
    "    labels = sorted({expected for expected, _ in pairs} | {predicted for _, predicted in pairs})\n",
    "    label_index = {label: position for position, label in enumerate(labels)}\n",
    "    matrix = [[0 for _ in labels] for _ in labels]\n",
    "    for expected, predicted in pairs:\n",
    "        row = label_index[predicted]\n",
    "        column = label_index[expected]\n",
    "        matrix[row][column] += 1\n",
    "    return labels, matrix\n",
    "\n",
    "matrix_labels, confusion = build_confusion_matrix(evaluation_pairs)\n",
    "print(\"Labels (in order):\", matrix_labels)\n",
    "print(\"Confusion matrix (rows = predicted, columns = actual):\")\n",
    "for row in confusion:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a manual prediction\n",
    "To see the algorithm in action I pick a flower with long petals and check the predicted species."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "mystery_flower = [6.1, 2.8, 4.7, 1.4]\nprediction = knn_predict(train_data, mystery_flower)\nprint(f\"Predicted species: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small conclusions\n",
    "- Reading the CSV myself made me notice how the raw numbers line up with the theory columns.\n",
    "- The evaluation step shows if my simple intuition is on the right track.\n",
    "- With more data I could tune *k* or try other algorithms from the course notes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}